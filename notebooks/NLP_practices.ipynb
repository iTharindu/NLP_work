{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_practices.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tki_LZEGw5U5",
        "colab_type": "text"
      },
      "source": [
        "**<h2>Tokenizer<h2>**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6nBiCpfuGDv",
        "colab_type": "code",
        "outputId": "6ff9916f-2959-482a-f55e-85ce16a4ef36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "example1 = nlp(\"This is an example\")\n",
        "\n",
        "for token in example1:\n",
        "  print(token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This\n",
            "is\n",
            "an\n",
            "example\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzQRZM4WulIX",
        "colab_type": "code",
        "outputId": "b31ba170-64b4-4395-ab50-7a35bf868871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "example2 = nlp(\"This is an example of text we're considering working for google.Are you also considering the same thing,Naomi?\")\n",
        "\n",
        "for token in example2:\n",
        "  print(token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This\n",
            "is\n",
            "an\n",
            "example\n",
            "of\n",
            "text\n",
            "we\n",
            "'re\n",
            "considering\n",
            "working\n",
            "for\n",
            "google\n",
            ".\n",
            "Are\n",
            "you\n",
            "also\n",
            "considering\n",
            "the\n",
            "same\n",
            "thing\n",
            ",\n",
            "Naomi\n",
            "?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDszHvIAvi5R",
        "colab_type": "code",
        "outputId": "0fa95783-023c-4528-9a7c-d98a3b6bc9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "text = \"This is an example of text we're considering working for google.Are you also considering the same thing,Naomi?\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'text',\n",
              " 'we',\n",
              " \"'re\",\n",
              " 'considering',\n",
              " 'working',\n",
              " 'for',\n",
              " 'google.Are',\n",
              " 'you',\n",
              " 'also',\n",
              " 'considering',\n",
              " 'the',\n",
              " 'same',\n",
              " 'thing',\n",
              " ',',\n",
              " 'Naomi',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGAuGsSkvno5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0EcGx3aPfx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = [stemmer.stem(token) for token in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M39AHDo4P0KW",
        "colab_type": "code",
        "outputId": "2b1f1274-66ef-4125-ac0c-b6c99b2d6c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\" \".join(example))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thi is an exampl of text we 're consid work for google.ar you also consid the same thing , naomi ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGHTTse4P4T-",
        "colab_type": "code",
        "outputId": "aa5f1ef8-d5e8-4953-d6a5-f2a52b759337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "for token in example2:\n",
        "  print(token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this\n",
            "be\n",
            "an\n",
            "example\n",
            "of\n",
            "text\n",
            "-PRON-\n",
            "be\n",
            "consider\n",
            "work\n",
            "for\n",
            "google\n",
            ".\n",
            "be\n",
            "-PRON-\n",
            "also\n",
            "consider\n",
            "the\n",
            "same\n",
            "thing\n",
            ",\n",
            "Naomi\n",
            "?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPqLKOB6QLYx",
        "colab_type": "code",
        "outputId": "0ea03813-3a92-4c08-9f80-570eabf1c24a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "example3 = nlp(\"is are am\")\n",
        "\n",
        "for token in example3:\n",
        "  print(token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "be\n",
            "be\n",
            "be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTJU_nnoQlMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(binary = True, token_pattern = r'\\b[^\\d\\W]+\\b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKFsb6e7RIQJ",
        "colab_type": "code",
        "outputId": "c2fe5c40-7a7f-4a91-e65e-77517ea00211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "corpus = [\"I am Jack\",\"You are John\", \"She is Naomi\"]\n",
        "\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "print(vectorizer.transform(corpus).toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 1 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 1 0 0 1]\n",
            " [0 0 0 1 0 0 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GMXVZp-RhLN",
        "colab_type": "code",
        "outputId": "e4207d5f-e7ef-48c2-ee9b-7b924cc14bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "vocab = vectorizer.vocabulary_\n",
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'am': 0,\n",
              " 'are': 1,\n",
              " 'i': 2,\n",
              " 'is': 3,\n",
              " 'jack': 4,\n",
              " 'john': 5,\n",
              " 'naomi': 6,\n",
              " 'she': 7,\n",
              " 'you': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sANT7C4WRl_t",
        "colab_type": "code",
        "outputId": "250b412d-c517-4b62-c626-25ee5eb3e417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        " example = \"man woman king queen\"\n",
        "\n",
        "tokens = nlp(example)\n",
        "for token1 in tokens :\n",
        "  for token2 in tokens:\n",
        "    print(token.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "am man 1.0\n",
            "am woman 0.63921684\n",
            "am king 0.5535262\n",
            "am queen 0.18746983\n",
            "am man 0.63921684\n",
            "am woman 1.0\n",
            "am king 0.6757708\n",
            "am queen 0.26638454\n",
            "am man 0.5535262\n",
            "am woman 0.6757708\n",
            "am king 1.0\n",
            "am queen 0.3645325\n",
            "am man 0.18746983\n",
            "am woman 0.26638454\n",
            "am king 0.3645325\n",
            "am queen 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVa0ZurtOw-I",
        "colab_type": "code",
        "outputId": "b8f4b138-77a2-432e-d496-fcb18b255937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "example = \"Google is founded by Larry Page and Sergey Brin in USA\"\n",
        "\n",
        "doc = nlp(example)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google ORG\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "USA GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C42nCEZ_QzfF",
        "colab_type": "code",
        "outputId": "c9a8221d-3ae3-4fbb-ac1c-6bd990a53731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-deIkFlP9x2",
        "colab_type": "code",
        "outputId": "b77fc6fe-38ee-43b2-ce0c-2fa33e5007a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentences = [\n",
        "             \"This is an awesome movie\",\n",
        "             \"I love it\",\n",
        "             \"This is my house\",\n",
        "             \"Your product is the worst one yet\",\n",
        "             \"Your product isn't that good\",\n",
        "             \"This sucks\",\n",
        "             \"I fucking love it\"\n",
        "]\n",
        "\n",
        "for sentence in sentences :\n",
        "  print(sentence, analyzer.polarity_scores(sentence)['compound'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is an awesome movie 0.6249\n",
            "I love it 0.6369\n",
            "This is my house 0.0\n",
            "Your product is the worst one yet -0.6249\n",
            "Your product isn't that good -0.3412\n",
            "This sucks -0.3612\n",
            "I fucking love it 0.6697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXf536zkQxT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}